{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d21990",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies\n",
    "!apt-get update -qq\n",
    "!apt-get install -y espeak-ng\n",
    "!pip install -q TTS librosa soundfile viphoneme\n",
    "\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30a1a24",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2 (FIXED): VietSpeech Streaming Dataset - Tr√°nh OOM\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import pyarrow.parquet as pq\n",
    "from huggingface_hub import hf_hub_download\n",
    "import soundfile as sf\n",
    "from io import BytesIO\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class VietSpeechStreamingDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Streaming dataset cho 100GB VietSpeech\n",
    "    ‚úÖ ƒê·ªçc parquet theo BATCH nh·ªè (tr√°nh OOM)\n",
    "    ‚úÖ Download file ‚Üí Process theo batch ‚Üí Yield samples ‚Üí Delete cache\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_files=27, max_duration=10.0, target_sr=22050, \n",
    "                 parquet_batch_size=512):\n",
    "        self.num_files = num_files\n",
    "        self.max_duration = max_duration\n",
    "        self.target_sr = target_sr\n",
    "        self.parquet_batch_size = parquet_batch_size  # ƒê·ªçc 512 rows/l·∫ßn\n",
    "        self.repo_id = \"NhutP/VietSpeech\"\n",
    "    \n",
    "    def __iter__(self):\n",
    "        import librosa\n",
    "        \n",
    "        for file_idx in range(self.num_files):\n",
    "            filename = f\"data/train-{file_idx:05d}-of-00027.parquet\"\n",
    "            parquet_file = None\n",
    "            \n",
    "            try:\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"üì• File {file_idx+1}/{self.num_files}: {filename}\")\n",
    "                print(f\"{'='*60}\")\n",
    "                \n",
    "                # Download file (~4GB)\n",
    "                parquet_file = hf_hub_download(\n",
    "                    repo_id=self.repo_id,\n",
    "                    filename=filename,\n",
    "                    repo_type=\"dataset\"\n",
    "                )\n",
    "                \n",
    "                print(f\"  ‚úÖ Downloaded: {os.path.basename(parquet_file)}\")\n",
    "                \n",
    "                # ==========================================\n",
    "                # üî• ƒê·ªåC THEO BATCH (TR√ÅNH OOM)\n",
    "                # ==========================================\n",
    "                # Thay v√¨: table = pq.read_table() (load h·∫øt 4.8GB v√†o RAM)\n",
    "                # D√πng: iter_batches() ƒë·ªÉ ƒë·ªçc t·ª´ng batch nh·ªè\n",
    "                \n",
    "                parquet_file_obj = pq.ParquetFile(parquet_file)\n",
    "                total_rows = parquet_file_obj.metadata.num_rows\n",
    "                \n",
    "                print(f\"  üìä Total rows: {total_rows:,}\")\n",
    "                print(f\"  üì¶ Reading in batches of {self.parquet_batch_size}\")\n",
    "                \n",
    "                valid_count = 0\n",
    "                processed_rows = 0\n",
    "                \n",
    "                # Iterate theo batch\n",
    "                for batch in parquet_file_obj.iter_batches(\n",
    "                    batch_size=self.parquet_batch_size\n",
    "                ):\n",
    "                    # Convert batch to dict\n",
    "                    batch_dict = batch.to_pydict()\n",
    "                    batch_size = len(batch_dict['audio'])\n",
    "                    \n",
    "                    # Process t·ª´ng row trong batch\n",
    "                    for i in range(batch_size):\n",
    "                        try:\n",
    "                            # Get audio\n",
    "                            audio_data = batch_dict['audio'][i]\n",
    "                            audio_bytes = audio_data['bytes']\n",
    "                            array, sr = sf.read(BytesIO(audio_bytes))\n",
    "                            \n",
    "                            # Filter duration\n",
    "                            duration = len(array) / sr\n",
    "                            if duration < 1.0 or duration > self.max_duration:\n",
    "                                continue\n",
    "                            \n",
    "                            # Resample to 22050 Hz\n",
    "                            if sr != self.target_sr:\n",
    "                                array = librosa.resample(\n",
    "                                    array,\n",
    "                                    orig_sr=sr,\n",
    "                                    target_sr=self.target_sr\n",
    "                                )\n",
    "                            \n",
    "                            # Normalize\n",
    "                            max_val = np.max(np.abs(array))\n",
    "                            if max_val > 0:\n",
    "                                array = array / max_val\n",
    "                            \n",
    "                            # Get text\n",
    "                            text = batch_dict.get('transcription', [None])[i] or batch_dict.get('text', [None])[i]\n",
    "                            if isinstance(text, list):\n",
    "                                text = text[0] if text else \"\"\n",
    "                            text = text.strip()\n",
    "                            \n",
    "                            # Filter text\n",
    "                            if len(text) < 5 or len(text) > 200:\n",
    "                                continue\n",
    "                            \n",
    "                            valid_count += 1\n",
    "                            \n",
    "                            yield {\n",
    "                                'audio': array,\n",
    "                                'text': text,\n",
    "                                'sampling_rate': self.target_sr,\n",
    "                                'duration': duration\n",
    "                            }\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            # Skip bad samples\n",
    "                            continue\n",
    "                    \n",
    "                    processed_rows += batch_size\n",
    "                    \n",
    "                    # Progress update every few batches\n",
    "                    if processed_rows % 5000 == 0:\n",
    "                        print(f\"  ‚è≥ Processed {processed_rows:,}/{total_rows:,} rows \"\n",
    "                              f\"({processed_rows/total_rows*100:.1f}%) | \"\n",
    "                              f\"Valid: {valid_count:,}\")\n",
    "                \n",
    "                print(f\"  ‚úÖ File complete: {valid_count:,} valid samples\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                \n",
    "            finally:\n",
    "                # X√ìA CACHE\n",
    "                if parquet_file and os.path.exists(parquet_file):\n",
    "                    try:\n",
    "                        os.remove(parquet_file)\n",
    "                        print(f\"  üóëÔ∏è Deleted cache\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ‚ö†Ô∏è Cannot delete: {e}\")\n",
    "                \n",
    "                # Force garbage collection\n",
    "                import gc\n",
    "                gc.collect()\n",
    "\n",
    "print(\"‚úÖ VietSpeechStreamingDataset created!\")\n",
    "print(\"\\nüìä Memory-efficient features:\")\n",
    "print(\"  ‚úì Reads parquet in batches (512 rows/time)\")\n",
    "print(\"  ‚úì Max RAM usage: ~50-100 MB per batch\")\n",
    "print(\"  ‚úì Deletes cache after each file\")\n",
    "print(\"  ‚úì Suitable for Kaggle (30GB RAM limit)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80653a60",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3: Load pretrained VITS from Coqui\n",
    "from TTS.api import TTS\n",
    "import torch\n",
    "\n",
    "print(\"üì• Loading pretrained VITS...\\n\")\n",
    "\n",
    "tts = TTS(\n",
    "    model_name=\"tts_models/en/ljspeech/vits\",\n",
    "    progress_bar=True,\n",
    "    gpu=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "vits_model = tts.synthesizer.tts_model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"‚úÖ VITS loaded: {type(vits_model).__name__}\")\n",
    "print(f\"‚úÖ Device: {device}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
